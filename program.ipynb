{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Conv2D, Dense, LeakyReLU, MaxPooling2D, BatchNormalization, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_name = 'cnn_for_asl_grayscale'\n",
    "already_used_images = []\n",
    "labels_dict ={\n",
    "    \"A\": 0,\n",
    "    \"B\": 1,\n",
    "    \"C\": 2,\n",
    "    \"D\": 3,\n",
    "    \"del\": 4,\n",
    "    \"E\": 5,\n",
    "    \"F\": 6,\n",
    "    \"G\": 7,\n",
    "    \"H\": 8,\n",
    "    \"I\": 9,\n",
    "    \"J\": 10,\n",
    "    \"K\": 11,\n",
    "    \"L\": 12,\n",
    "    \"M\": 13,\n",
    "    \"N\": 14,\n",
    "    \"nothing\": 15,\n",
    "    \"O\": 16,\n",
    "    \"P\": 17,\n",
    "    \"Q\": 18,\n",
    "    \"R\": 19,\n",
    "    \"S\": 20,\n",
    "    \"space\": 21,\n",
    "    \"T\": 22,\n",
    "    \"U\": 23,\n",
    "    \"V\": 24,\n",
    "    \"W\": 25,\n",
    "    \"X\": 26,\n",
    "    \"Y\": 27,\n",
    "    \"Z\": 28,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
    "    img = np.reshape(img, (img.shape[0], img.shape[1], 1))\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels_to_number(labels):\n",
    "    numbers_array = []\n",
    "    for label in labels:\n",
    "        numbers_array.append(labels_dict[label])\n",
    "    return numbers_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANTE** La variable asl_directory hace referencia a la ruta relativa en la que se encuentra la carpeta con todos los ejemplos de entrenamiento es necesario editarla de acuerdo a esta ubicación para que el programa pueda cargar los ejemplos de entrenamiento \\\n",
    "**IMPORTANT** The variable asl_directory makes reference to the relative route where all the training examples are located, this route needs to be changed according to this ubication in order for the program to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_train_samples():\n",
    "    asl_directory = '../asl-alphabet/asl_alphabet_train/asl_alphabet_train'\n",
    "    max_images_per_label = 500\n",
    "    \n",
    "    print('Already processed %i images' % len(already_used_images))\n",
    "    print('Loading images....')\n",
    "    absolute_path = os.path.abspath(asl_directory)\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for label in os.listdir(absolute_path):\n",
    "        \n",
    "        added_elements_per_label = 0\n",
    "        label_path = os.path.join(absolute_path, label)\n",
    "        \n",
    "        for image in os.listdir(label_path):\n",
    "            \n",
    "            if(added_elements_per_label >= max_images_per_label):\n",
    "                break;\n",
    "            \n",
    "            image_path = os.path.join(label_path, image)\n",
    "            \n",
    "            if(image_path not in already_used_images):\n",
    "            \n",
    "                img = load_image(image_path)\n",
    "                img = np.array(img,  dtype=np.uint8)\n",
    "\n",
    "                x.append(img)\n",
    "                y.append(label)\n",
    "                \n",
    "                added_elements_per_label = added_elements_per_label + 1\n",
    "                already_used_images.append(image_path)\n",
    "        \n",
    "    print('Loaded a total of %i images' % len(x))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANTE** La variable asl_test_directory hace referencia a la ruta relativa en la que se encuentra la carpeta con todos los ejemplos de prueba, es necesario editarla de acuerdo a esta ubicación para que el programa pueda cargar los ejemplos de entrenamiento \\\n",
    "**IMPORTANT** The variable asl_test_directory makes reference to the relative route where all the test examples are located, this route needs to be changed according to this ubication in order for the program to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_samples():\n",
    "    asl_test_directory = '../asl-alphabet/asl_alphabet_test/asl_alphabet_test'\n",
    "    absolute_path = os.path.abspath(asl_test_directory)\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for label in os.listdir(absolute_path):\n",
    "        \n",
    "        image_path = os.path.join(absolute_path, label)\n",
    "        \n",
    "        img = load_image(image_path)\n",
    "        \n",
    "        processed_label = label.split('.')[0].split('_')[0]\n",
    "        \n",
    "        x.append(img)\n",
    "        y.append(processed_label)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(input_shape, output_shape):\n",
    "    cnn_asl = Sequential()\n",
    "\n",
    "    cnn_asl.add(Conv2D(16,\n",
    "                       kernel_size=(16, 16),\n",
    "                       activation='relu',\n",
    "                       use_bias=True,\n",
    "                       input_shape=input_shape))\n",
    "    cnn_asl.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    cnn_asl.add(Conv2D(32,\n",
    "                       kernel_size=(8,8),\n",
    "                       activation='relu',\n",
    "                       use_bias=True))\n",
    "    cnn_asl.add(Conv2D(64,\n",
    "                       kernel_size=(8, 8),\n",
    "                       activation='relu',\n",
    "                       use_bias=True))\n",
    "    cnn_asl.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    cnn_asl.add(Flatten())\n",
    "    cnn_asl.add(BatchNormalization())\n",
    "    cnn_asl.add(Dense(128,\n",
    "                      activation='linear',\n",
    "                      use_bias=True))\n",
    "    cnn_asl.add(LeakyReLU(alpha=0.01))\n",
    "    cnn_asl.add(Dense(output_shape,\n",
    "                      activation='softmax',\n",
    "                     use_bias=True))\n",
    "    \n",
    "    return cnn_asl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_samples(x, y):\n",
    "    x = np.array(x, dtype=np.uint8)\n",
    "    y = map_labels_to_number(y)\n",
    "    y = to_categorical(y, len(labels_dict))\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, saved_model_name, x_train, y_train):\n",
    "    while(len(x_train)>0):\n",
    "        print('Running iteration')\n",
    "        model_string = '%s.h5' % saved_model_name\n",
    "        model.fit(x_train, y_train, batch_size=64, epochs=5)\n",
    "        model.save(model_string)\n",
    "\n",
    "        x_train, y_train = load_train_samples()\n",
    "        x_train, y_train = process_samples(x_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_model():\n",
    "    x_train, y_train = load_train_samples()\n",
    "    x_train, y_train = process_samples(x_train, y_train)\n",
    "    cnn_asl_model = generate_model(x_train[0].shape, y_train.shape[1])\n",
    "    cnn_asl_model.compile(optimizer=Adam(), loss=categorical_crossentropy, metrics=['accuracy'])\n",
    "    cnn_asl_model.summary()\n",
    "    cnn_asl_model = train_model(cnn_asl_model, saved_model_name, x_train, y_train)\n",
    "    return cnn_asl_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_test_model():\n",
    "    cnn_asl_model = load_model(saved_model_name + '.h5')\n",
    "    return test_model(cnn_asl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rectangle_coordinates(x_center, y_center, frame_height, frame_width):\n",
    "    first_coordinate = (int(x_center - frame_width / 2), int(y_center - frame_height / 2))\n",
    "    second_coordinate = (int(x_center + frame_width / 2), int(y_center + frame_height / 2))\n",
    "    \n",
    "    return first_coordinate, second_coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rectangle_frame(image, first_coordinate, second_coordinate, color):\n",
    "    image = cv.rectangle(image, first_coordinate, second_coordinate, color, 2)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_categorical_to_label(categorical_results):\n",
    "    categorical_results = np.argmax(categorical_results, axis=-1)\n",
    "    position = categorical_results[0]\n",
    "    keys_list = list(labels_dict.keys())\n",
    "    return keys_list[position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(cnn_asl_model):\n",
    "    x_test, y_test = load_test_samples()\n",
    "    x_test, y_test = process_samples(x_test, y_test)\n",
    "    results = cnn_asl_model.evaluate(x_test, y_test, verbose=1)\n",
    "    return cnn_asl_model, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente bloque de código se ejecuta en caso de que se desee crear y entrenar un modelo nuevo con los parámetros establecidos en la función `generate_model` \\\n",
    "The next code block is executed in case that a new model wants to be created and trained acording to the architecture stablished on the function `generate_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already processed 0 images\n",
      "Loading images....\n",
      "Loaded a total of 14500 images\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 185, 185, 16)      12304     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 61, 61, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 54, 54, 32)        32800     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 47, 47, 64)        131136    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 14400)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14400)             57600     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1843328   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 29)                3741      \n",
      "=================================================================\n",
      "Total params: 2,080,909\n",
      "Trainable params: 2,052,109\n",
      "Non-trainable params: 28,800\n",
      "_________________________________________________________________\n",
      "Running iteration\n",
      "Epoch 1/5\n",
      "14500/14500 [==============================] - 23s 2ms/step - loss: 1.8393 - accuracy: 0.5072\n",
      "Epoch 2/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.5871 - accuracy: 0.8272\n",
      "Epoch 3/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.2458 - accuracy: 0.9275\n",
      "Epoch 4/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.1487 - accuracy: 0.9584\n",
      "Epoch 5/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.1339 - accuracy: 0.9584\n",
      "Already processed 14500 images\n",
      "Loading images....\n",
      "Loaded a total of 14500 images\n",
      "Running iteration\n",
      "Epoch 1/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.7192 - accuracy: 0.7996\n",
      "Epoch 2/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.1087 - accuracy: 0.9692\n",
      "Epoch 3/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.0522 - accuracy: 0.9889\n",
      "Epoch 4/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.0305 - accuracy: 0.9930\n",
      "Epoch 5/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.0182 - accuracy: 0.9965\n",
      "Already processed 29000 images\n",
      "Loading images....\n",
      "Loaded a total of 14500 images\n",
      "Running iteration\n",
      "Epoch 1/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 1.8729 - accuracy: 0.5083\n",
      "Epoch 2/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.5326 - accuracy: 0.8363\n",
      "Epoch 3/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.2381 - accuracy: 0.9344\n",
      "Epoch 4/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.1454 - accuracy: 0.9630\n",
      "Epoch 5/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.0933 - accuracy: 0.9770\n",
      "Already processed 43500 images\n",
      "Loading images....\n",
      "Loaded a total of 14500 images\n",
      "Running iteration\n",
      "Epoch 1/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 1.1241 - accuracy: 0.6952\n",
      "Epoch 2/5\n",
      "14500/14500 [==============================] - 19s 1ms/step - loss: 0.2451 - accuracy: 0.9243\n",
      "Epoch 3/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.1466 - accuracy: 0.9537\n",
      "Epoch 4/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.0857 - accuracy: 0.9745\n",
      "Epoch 5/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.0626 - accuracy: 0.9814\n",
      "Already processed 58000 images\n",
      "Loading images....\n",
      "Loaded a total of 14500 images\n",
      "Running iteration\n",
      "Epoch 1/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.6874 - accuracy: 0.8148\n",
      "Epoch 2/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.0859 - accuracy: 0.9768\n",
      "Epoch 3/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.0352 - accuracy: 0.9914\n",
      "Epoch 4/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.0181 - accuracy: 0.9970\n",
      "Epoch 5/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.0119 - accuracy: 0.9987\n",
      "Already processed 72500 images\n",
      "Loading images....\n",
      "Loaded a total of 14500 images\n",
      "Running iteration\n",
      "Epoch 1/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.2973 - accuracy: 0.9229\n",
      "Epoch 2/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.0247 - accuracy: 0.9939\n",
      "Epoch 3/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.0099 - accuracy: 0.9977\n",
      "Epoch 4/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.0062 - accuracy: 0.9990\n",
      "Epoch 5/5\n",
      "14500/14500 [==============================] - 20s 1ms/step - loss: 0.0044 - accuracy: 0.9992\n",
      "Already processed 87000 images\n",
      "Loading images....\n"
     ]
    }
   ],
   "source": [
    "cnn_asl_model = create_and_train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_asl_model,results = test_model(cnn_asl_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de que se desee cargar un modelo ya guardado en la misma carpeta que el archivo se ejecuta el siguiente bloque de código, este bloque carga el modelo que tenga el mismo nombre que la variable `saved_model_name` con un formato .h5 \\\n",
    "In case that a model already saved on the same folder wants to be loaded the next code block must be executed, this block loads the model that has the same name saved on the variable `saved_model_name` with a format .h5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "cnn_asl_model,results = load_and_test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.527571678161621, 0.9642857313156128]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente bloque de código puede ser utlizado para probar el modelo con la cámara del computador \\\n",
    "The next code block could be used to test the model with the computers camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vid = cv.VideoCapture(0)\n",
    "\n",
    "x_center = vid.get(3) / 2\n",
    "y_center = vid.get(4) / 2\n",
    "\n",
    "frame_height = 200\n",
    "frame_width = 200\n",
    "\n",
    "first_coordinate, second_coordinate = compute_rectangle_coordinates(x_center, y_center, frame_height, frame_width)\n",
    "  \n",
    "while(True):\n",
    "      \n",
    "    # Capture the video frame\n",
    "    # by frame\n",
    "    ret, frame = vid.read()\n",
    "\n",
    "    cropped_frame = frame[first_coordinate[1]: first_coordinate[1]+frame_height, first_coordinate[0]: first_coordinate[0]+frame_width]\n",
    "    cropped_frame = cv.cvtColor(cropped_frame, cv.COLOR_BGR2GRAY)\n",
    "    processed_cropped_frame = np.reshape(cropped_frame, (1, cropped_frame.shape[0], cropped_frame.shape[1], 1))\n",
    "\n",
    "    frame = draw_rectangle_frame(frame, first_coordinate, second_coordinate, (252, 19, 3))\n",
    "\n",
    "    results = cnn_asl_model.predict(processed_cropped_frame)\n",
    "    label = map_categorical_to_label(results)\n",
    "    \n",
    "    frame = cv.putText(frame, 'The predicted value is: ' + label, (int(0), int(y_center*2)), cv.FONT_HERSHEY_SIMPLEX, int(1), (int(255),int(255),int(255)), 2)\n",
    "    # Display the resulting frame\n",
    "    cv.imshow('frame', cropped_frame)\n",
    "    cv.imshow('frame2', frame)\n",
    "      \n",
    "    # the 'q' button is set as the\n",
    "    # quitting button you may use any\n",
    "    # desired button of your choice\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "  \n",
    "# After the loop release the cap object\n",
    "vid.release()\n",
    "# Destroy all the windows\n",
    "cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
